import jax
import jax.numpy as jnp
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import gen_batches
from tqdm import tqdm, trange
from .base import BaseModel
from .logging import Logger
from .visualization import plot_curves


class Trainer:
    def __init__(self, model: BaseModel, optimizer, loss_fun, metric_fun,
                 n_samples: int, batch_size: int, n_epochs: int, train_split: float,
                 optimizer_params: dict = None):
        self.model = model
        self.optimizer = optimizer(**optimizer_params)
        self.loss = self.model.compute_loss(loss_fun)
        self.eval = metric_fun
        self.params = {'batch_size': batch_size, 'n_epochs': n_epochs,
                       'train_split': train_split, }
        self.train_samples, self.test_samples = train_test_split(np.arange(n_samples),
                                                                 train_size=train_split)
        self.batch_bins = list(gen_batches(int(n_samples * train_split), batch_size))
        self.logs = Logger(n_epochs, batch_size)
        self.gradients = []

    def train(self, X: jnp.ndarray, y, **kwargs):
        self.logs.reset()
        model_params = self.model.parameters
        opt_state = self.optimizer.init(model_params)
        # TODO: loop over epochs and batches
        # TODO: can the same be generated by a lax.fori_loop statement?
        for i in trange(self.params['n_epochs'], desc="Epochs"):
            epoch_loss = []
            epoch_metric = []
            # random shuffle of training sample order
            # => random minibatches for each epoch
            np.random.shuffle(self.train_samples)
            for batch in tqdm(self.batch_bins, desc="Mini-Batches"):
                # z, distort = train_func(model_params, self.model.layers, X[:, self.train_samples[batch]])
                loss, gradients = jax.value_and_grad(self.loss)(model_params, X[:, self.train_samples[batch]], **kwargs)
                self.gradients.append(gradients)
                epoch_loss.append(loss)
                if y is not None:
                    epoch_metric.append(self.eval(X[:, batch], y[batch]))
                model_params, opt_state = self.optimizer.update(gradients, opt_state)
            if y is not None:
                self.logs.update(
                    epoch_loss, epoch_metric,
                    self.eval(X[:, self.test_samples], y[self.test_samples]),
                    self.loss(X[:, self.test_samples], y[self.test_samples])
                )
            else:
                self.logs.update(
                    epoch_loss, epoch_metric,
                    self.eval(X[:, self.test_samples], y=y),
                    self.loss(model_params, X[:, self.test_samples], y=y)
                )
        self.model.update_parameters(model_params)

    # def lax_train(self, X: jnp.ndarray, y):
    #     def _train_(state):
    #         z = self.model.train(X)
    #         loss, gradients = jax.value_and_grad(self.loss)(z, y)
    #         updates, new_state = self.optimizer.update(gradients, state)
    #         self.model.update_parameters(updates)
    #         return new_state
    #     opt_state = self.optimizer.init(self.model.parameters)
    #     lax.fori_loop(0, self.train_settings['n_epochs'], _train_, opt_state)

    def plot_training_curves(self, **kwargs):
        plot_curves(self.logs, self.eval.__name__, **kwargs)
